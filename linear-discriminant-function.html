<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-flash.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"hasegawaazusa.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true}}</script><script src="/js/config.js"></script>

    <meta name="description" content="有关线性判别函数的简要笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="模式识别-线性判别函数笔记">
<meta property="og:url" content="https://hasegawaazusa.github.io/linear-discriminant-function.html">
<meta property="og:site_name" content="独奏の小屋">
<meta property="og:description" content="有关线性判别函数的简要笔记">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%83%85%E5%86%B5%E4%B8%80%E4%B8%8B%E7%9A%84%E5%A4%9A%E7%B1%BB%E9%97%AE%E9%A2%98%E5%88%86%E7%B1%BB%E7%95%8C%E9%9D%A2.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%83%85%E5%86%B5%E4%BA%8C%E4%B8%8B%E7%9A%84%E5%A4%9A%E7%B1%BB%E9%97%AE%E9%A2%98%E5%88%86%E7%B1%BB%E7%95%8C%E9%9D%A2.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%83%85%E5%86%B5%E4%B8%89%E4%B8%8B%E7%9A%84%E5%A4%9A%E7%B1%BB%E9%97%AE%E9%A2%98%E5%88%86%E7%B1%BB%E7%95%8C%E9%9D%A2.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E4%B8%80%E4%B8%AA%E4%B8%89%E7%BB%B4%E5%A2%9E%E5%B9%BF%E5%8C%96%E5%90%8E%E7%9A%84%E7%89%B9%E5%BE%81%E7%A9%BA%E9%97%B4.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E9%9D%9E%E8%A7%84%E8%8C%83%E5%8C%96%E6%A0%B7%E6%9C%AC.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E8%A7%84%E8%8C%83%E5%8C%96%E6%A0%B7%E6%9C%AC.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%9C%80%E5%B0%91%E9%94%99%E5%88%86%E6%A0%B7%E6%9C%AC%E6%95%B0%E5%87%86%E5%88%99%E4%B8%8B%E7%9A%84%E5%87%86%E5%88%99%E5%87%BD%E6%95%B0.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E4%BA%8C%E7%BB%B4%E4%BA%8C%E7%B1%BB%E6%9D%83%E7%A9%BA%E9%97%B4%E7%9A%84%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E4%BA%8C%E7%BB%B4%E4%BA%8C%E7%B1%BB%E6%9D%83%E7%A9%BA%E9%97%B4%E7%9A%84%E5%9B%BA%E5%AE%9A%E5%A2%9E%E9%87%8F%E6%B3%95.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E4%BA%8C%E7%BB%B4%E4%BA%8C%E7%B1%BB%E6%9D%83%E7%A9%BA%E9%97%B4%E7%9A%84%E5%B8%A6%E8%A3%95%E9%87%8F%E7%9A%84%E5%9B%BA%E5%AE%9A%E5%A2%9E%E9%87%8F%E6%B3%95.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E5%90%84%E5%90%91%E9%87%8F%E5%AF%B9%E6%9D%83%E5%90%91%E9%87%8F%E7%9A%84%E6%8A%95%E5%BD%B1.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/Fisher%E5%87%86%E5%88%99%E4%B8%80%E7%BB%B4%E5%8C%96.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/fisher%E5%87%86%E5%88%99%E7%A4%BA%E4%BE%8B%E5%9B%BE%E6%A0%87.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E5%88%86%E7%B1%BB%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%A4%9A%E7%A7%8D%E8%B6%85%E5%B9%B3%E9%9D%A2.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%94%AF%E6%92%91%E5%90%91%E9%87%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E5%88%A4%E5%86%B3%E5%87%BD%E6%95%B0%E7%9A%84%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%80%A7%E8%B4%A8.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E8%A7%A3.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%A0%B8%E6%98%A0%E5%B0%84%E7%9A%84%E4%B8%80%E4%B8%AA%E7%A4%BA%E6%84%8F%E5%9B%BE.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E4%B8%80%E4%B8%AA%E4%BA%8C%E7%BB%B4%E5%88%B0%E4%B8%89%E7%BB%B4%E7%9A%84%E6%A0%B8%E6%98%A0%E5%B0%84.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E8%AE%BE%E8%AE%A1%E5%AE%9E%E4%BE%8B.png">
<meta property="og:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0.png">
<meta property="article:published_time" content="2023-04-03T11:00:00.000Z">
<meta property="article:modified_time" content="2023-06-25T13:30:17.760Z">
<meta property="article:author" content="qsdz">
<meta property="article:tag" content="模式识别">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hasegawaazusa.github.io/linear-discriminant-function/%E6%83%85%E5%86%B5%E4%B8%80%E4%B8%8B%E7%9A%84%E5%A4%9A%E7%B1%BB%E9%97%AE%E9%A2%98%E5%88%86%E7%B1%BB%E7%95%8C%E9%9D%A2.png">


<link rel="canonical" href="https://hasegawaazusa.github.io/linear-discriminant-function.html">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://hasegawaazusa.github.io/linear-discriminant-function.html","path":"/linear-discriminant-function.html","title":"模式识别-线性判别函数笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>模式识别-线性判别函数笔记 | 独奏の小屋</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">独奏の小屋</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">60</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">112</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB"><span class="nav-number">1.</span> <span class="nav-text">模式识别</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E7%B1%BB%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.</span> <span class="nav-text">二类线性判别函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E7%B1%BB%E6%83%85%E5%86%B5%E4%B8%8B%E7%9A%84%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.</span> <span class="nav-text">多类情况下的线性判别函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%85%E5%86%B5%E4%B8%80"><span class="nav-number">1.2.1.</span> <span class="nav-text">情况一</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%85%E5%86%B5%E4%BA%8C"><span class="nav-number">1.2.2.</span> <span class="nav-text">情况二</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%83%85%E5%86%B5%E4%B8%89"><span class="nav-number">1.2.3.</span> <span class="nav-text">情况三</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.4.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.</span> <span class="nav-text">线性判别函数的学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A2%9E%E5%B9%BF%E5%8C%96"><span class="nav-number">1.3.1.</span> <span class="nav-text">增广化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%84%E8%8C%83%E5%8C%96"><span class="nav-number">1.3.2.</span> <span class="nav-text">规范化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="nav-number">1.3.3.</span> <span class="nav-text">学习算法原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.4.</span> <span class="nav-text">梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8A%A3"><span class="nav-number">1.3.4.3.</span> <span class="nav-text">优劣</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%9B%E9%A1%BF%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.3.5.</span> <span class="nav-text">牛顿下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8E%9F%E7%90%86-1"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8A%A3-1"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">优劣</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.3.6.</span> <span class="nav-text">感知器梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-1"><span class="nav-number">1.3.6.1.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BA%E5%AE%9A%E5%A2%9E%E9%87%8F%E6%B3%95"><span class="nav-number">1.3.7.</span> <span class="nav-text">固定增量法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-2"><span class="nav-number">1.3.7.1.</span> <span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E5%A2%9E%E9%87%8F%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.8.</span> <span class="nav-text">变增量梯度算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B8%A6%E8%A3%95%E9%87%8F%E7%9A%84%E6%84%9F%E7%9F%A5%E5%99%A8%E7%AE%97%E6%B3%95"><span class="nav-number">1.3.9.</span> <span class="nav-text">带裕量的感知器算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81-3"><span class="nav-number">1.3.9.1.</span> <span class="nav-text">代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#fisher-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB"><span class="nav-number">1.4.</span> <span class="nav-text">Fisher 线性判别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8E%9F%E7%90%86-2"><span class="nav-number">1.4.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B"><span class="nav-number">1.4.2.</span> <span class="nav-text">示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">1.5.</span> <span class="nav-text">支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="nav-number">1.5.1.</span> <span class="nav-text">支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E7%AD%89%E5%BC%8F%E7%BA%A6%E6%9D%9F%E7%9A%84%E6%9E%81%E5%B0%8F%E5%80%BC"><span class="nav-number">1.5.2.</span> <span class="nav-text">不等式约束的极小值</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%97%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AE%9A%E7%90%86"><span class="nav-number">1.5.2.1.</span> <span class="nav-text">朗格朗日定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kt-%E6%9C%80%E4%BC%98%E6%80%A7%E6%9D%A1%E4%BB%B6"><span class="nav-number">1.5.2.2.</span> <span class="nav-text">KT 最优性条件</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="nav-number">1.5.3.</span> <span class="nav-text">对偶问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-1"><span class="nav-number">1.5.4.</span> <span class="nav-text">支持向量机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="nav-number">1.5.5.</span> <span class="nav-text">核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="nav-number">1.5.6.</span> <span class="nav-text">核支持向量机</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="qsdz"
      src="/images/android-chrome-144x144.png">
  <p class="site-author-name" itemprop="name">qsdz</p>
  <div class="site-description" itemprop="description">又菜又爱玩，望轻喷</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">112</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">60</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/HasegawaAzusa" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HasegawaAzusa" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hasegawaazusa.github.io/linear-discriminant-function.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/android-chrome-144x144.png">
      <meta itemprop="name" content="qsdz">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="独奏の小屋">
      <meta itemprop="description" content="又菜又爱玩，望轻喷">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="模式识别-线性判别函数笔记 | 独奏の小屋">
      <meta itemprop="description" content="有关线性判别函数的简要笔记">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          模式识别-线性判别函数笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-04-03 19:00:00" itemprop="dateCreated datePublished" datetime="2023-04-03T19:00:00+08:00">2023-04-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-06-25 21:30:17" itemprop="dateModified" datetime="2023-06-25T21:30:17+08:00">2023-06-25</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>23k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>21 分钟</span>
    </span>
</div>

            <div class="post-description">有关线性判别函数的简要笔记</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="模式识别">模式识别</h1>
<h2 id="二类线性判别函数">二类线性判别函数</h2>
<p>线性判别函数有许多优势，其中一个优势是便于分析和计算。在之前的贝叶斯决策中笔记中曾简单涉及到的贝叶斯判别函数，<strong>当满足某些情况时，判别函数是线性的</strong>。</p>
<p>我们抽象出这种线性特质，即定义<strong>线性判别函数</strong>形如 <span class="math display">\[
g(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+w_0
\]</span> 其中，<span class="math inline">\(\boldsymbol{w}\)</span>
是<strong>权向量</strong>（weight
vector），决定了特征向量中每个属性在判别函数中的占比；<span class="math inline">\(w_0\)</span>
是<strong>偏置</strong>（bias），决定了该线性判别函数的位置。</p>
<p>在这其中，二类线性判别函数在实战中常被用到，故以下仅讨论<strong>二类线性判别函数</strong>的问题，其中可能往往都以<strong>二维二类线性判别函数</strong>为例。</p>
<p><strong>性质一：权向量 <span class="math inline">\(\boldsymbol{w}\)</span>
与判别面上任意一个向量正交，即权向量决定了判别边界的方向。</strong></p>
<blockquote>
<p>对于一个二维二类线性判别函数，假设判别面上存在任意两点 <span class="math inline">\(x_1,x_2\)</span>，那么其满足 <span class="math display">\[
\boldsymbol{w}^Tx_1+w_0=\boldsymbol{w}^Tx_2+w_0
\]</span> 可以改写成 <span class="math display">\[
\boldsymbol{w}^T(x_1-x_2)=0
\]</span> 即权向量与判别面上的任意一向量正交（相互垂直）。</p>
<p>可以推广到，权向量同时也与超平面（多维情况下的判别边界）上的任意向量正交。</p>
</blockquote>
<p><strong>性质二：判别函数 <span class="math inline">\(g(\boldsymbol{x})\)</span> 与特征向量 <span class="math inline">\(\boldsymbol{x}\)</span>
到判决边界（判决面）的距离成正比。</strong></p>
<blockquote>
<p>我们定义 <span class="math inline">\(r\)</span> 为特征空间中某点
<span class="math inline">\(\boldsymbol{x}\)</span>
到判决边界（超平面）的算数距离（正代表在超平面正侧，反之亦然）；使用
<span class="math inline">\(\frac{\boldsymbol{w}}{\|\boldsymbol{w}\|}\)</span>
来表示单位向量，那么特征向量 <span class="math inline">\(\boldsymbol{x}\)</span> 可以表示成 <span class="math display">\[
\boldsymbol{x}=\boldsymbol{x}_p+r\frac{\boldsymbol{w}}{\|\boldsymbol{w}\|}
\]</span> 其中 <span class="math inline">\(\boldsymbol{x}_p\)</span>
是特征向量在判决边界上的投影，有 <span class="math inline">\(g(\boldsymbol{x}_p)=0\)</span>。</p>
<p>代入判决函数，可以得到 <span class="math display">\[
\boldsymbol{w}^T\boldsymbol{x}+w_0=r\|\boldsymbol{w}\|
\]</span> 即 <span class="math display">\[
r=\frac{g(\boldsymbol{x})}{\|\boldsymbol{w}\|}
\]</span></p>
</blockquote>
<p>由<strong>性质二</strong>，我们定义</p>
<p><span class="math inline">\(g(\boldsymbol{x})&gt;0\)</span>
称作判决边界（判决面）的正面；<span class="math inline">\(g(\boldsymbol{x})&lt;0\)</span>
称作判决边界的反面；<span class="math inline">\(g(\boldsymbol{x})=0\)</span> 则 <span class="math inline">\(\boldsymbol{x}\)</span> 是在判决边界上。</p>
<p><strong>性质三：线性判别函数中的偏置 <span class="math inline">\(w_0\)</span> 表征了原点到判别边界的距离。若 <span class="math inline">\(w_0&gt;0\)</span>，则原点位于判别边界的正面；反之亦然。</strong></p>
<blockquote>
<p>我们显然可以得知 <span class="math display">\[
g(\boldsymbol{0})=w_0
\]</span> 故结论显然。</p>
</blockquote>
<h2 id="多类情况下的线性判别函数">多类情况下的线性判别函数</h2>
<h3 id="情况一">情况一</h3>
<p>如果每个模式类均可用一个单独的线性判别边界与其余模式类分开，那么此时仅需
<span class="math inline">\(c\)</span> 个判别函数，且具有以下性质：
<span class="math display">\[
\begin{cases}
g_i(\boldsymbol{x})&gt;0 &amp; \boldsymbol{x}\in\omega_i\\
g_i(\boldsymbol{x})&lt;0 &amp; \boldsymbol{x}\notin\omega_i
\end{cases}
\]</span> <img src="/linear-discriminant-function/情况一下的多类问题分类界面.png" alt="情况一下的多类问题分类界面"></p>
<p>当关于特征向量有 <span class="math inline">\(\boldsymbol{x}\)</span>
满足 <span class="math inline">\(g_i(\boldsymbol{x})&gt;0\)</span>
且所有 <span class="math inline">\(g_j{(\boldsymbol{x})}&lt;0\)</span>
时，判决为 <span class="math inline">\(\omega_i\)</span>；反之不作判决。</p>
<p>故可以发现，该方法存在<strong>失效区或不定区</strong>，在图中表现为<strong>模糊区域</strong>，即存在多于一个判别函数大于
0 或所有的判别函数都小于 0。</p>
<h3 id="情况二">情况二</h3>
<p>如果线性判别边界只能将模式类两两分开，那么需要 <span class="math inline">\(c(c-1)/2\)</span> 个判别函数，且具有以下性质 <span class="math display">\[
\begin{cases}
g_{ij}(\boldsymbol{x})&gt;0 &amp; \boldsymbol{x}\in\omega_i\\
g_{ij}(\boldsymbol{x})&lt;0 &amp; \boldsymbol{x}\in\omega_j
\end{cases}
\]</span> 显然，应有 <span class="math display">\[
g_{ij}(\boldsymbol{x})=-g_{ji}(\boldsymbol{x})
\]</span> <img src="/linear-discriminant-function/情况二下的多类问题分类界面.png" alt="情况二下的多类问题分类界面"></p>
<p>当特征向量 <span class="math inline">\(\boldsymbol{x}\)</span>
满足对所有的 <span class="math inline">\(j\neq i\)</span> 均有 <span class="math inline">\(g_{ij}(\boldsymbol{x})&gt;0\)</span>，那么判决为
<span class="math inline">\(\omega_i\)</span>，反之不判决。</p>
<p>同理，该种方式任然存在不定区，采用拒识策略。</p>
<blockquote>
<p>拒识策略即不进行判决。</p>
</blockquote>
<h3 id="情况三">情况三</h3>
<p>不考虑二类问题的线性判别函数，采用 <span class="math inline">\(c\)</span>
个线性判别函数进行分类，但不同于情况一，此时的识别准则变更为</p>
<p>对于所有的 <span class="math inline">\(i\neq j\)</span>，若 <span class="math inline">\(g_i(\boldsymbol{x})&gt;g_j(\boldsymbol{x})\)</span>，则判决为
<span class="math inline">\(\omega_i\)</span>。</p>
<p>该种方法实际上是将特征空间划分为 <span class="math inline">\(c\)</span> 个判决区域 <span class="math inline">\(\{R_1,R_2,\cdots,R_c\}\)</span>，在判决区域 <span class="math inline">\(R_i\)</span> 内，<span class="math inline">\(g_i(\boldsymbol{x})\)</span>
具有最大的函数值。</p>
<p>如果 <span class="math inline">\(R_i\)</span> 与 <span class="math inline">\(R_j\)</span> 相邻，则决策面是方程 <span class="math inline">\(g_i(\boldsymbol{x})=g_j(\boldsymbol{x})\)</span>
的一部分。</p>
<blockquote>
<p>因为决策面还有可能与其他模式冲突，所以这里描述为一部分。</p>
</blockquote>
<p>但该种方法的优点是不存在不定区。</p>
<figure>
<img src="/linear-discriminant-function/情况三下的多类问题分类界面.png" alt="情况三下的多类问题分类界面">
<figcaption aria-hidden="true">情况三下的多类问题分类界面</figcaption>
</figure>
<h3 id="总结">总结</h3>
<p>由上述情况可知，多类情况实际上可以转化为二类问题进行处理，故我们仅需研究二类问题中的线性判别函数即可。</p>
<h2 id="线性判别函数的学习算法">线性判别函数的学习算法</h2>
<h3 id="增广化">增广化</h3>
<p>我们知道线性判别函数一般具有以下形式 <span class="math display">\[
g(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+w_0=w_1x_1+w_2x_2+\cdots+w_dx_d+w_0
\]</span> 不妨我们设 <span class="math display">\[
\boldsymbol{a}=\begin{bmatrix}
w_0\\
w_1\\
\vdots\\
w_d
\end{bmatrix},\boldsymbol{y}=\begin{bmatrix}
1\\
x_1\\
\vdots\\
x_d
\end{bmatrix}
\]</span> 将原判别函数写成 <span class="math display">\[
g(\boldsymbol{y})=\boldsymbol{a}^T\boldsymbol{y}
\]</span> 其中 <span class="math inline">\(\boldsymbol{a},\boldsymbol{y}\)</span> 的维度均为
<span class="math inline">\(d+1\)</span> 维。</p>
<p>我们把这种化简称作<strong>增广化</strong>。</p>
<p>比如说我们有两类样本 <span class="math display">\[
\omega_1:\boldsymbol{x}_{11}=(0,0),\boldsymbol{x}_{12}=(0,1)\\
\omega_2:\boldsymbol{x}_{21}=(1,0),\boldsymbol{x}_{22}=(1,1)\\
\]</span> 我们对其进行增广化，得到 <span class="math display">\[
\omega_1:\boldsymbol{x}_{11}=(1,0,0),\boldsymbol{x}_{12}=(1,0,1)\\
\omega_2:\boldsymbol{x}_{21}=(1,1,0),\boldsymbol{x}_{22}=(1,1,1)\\
\]</span></p>
<blockquote>
<p>增广化的结果不唯一，比如说也可以存在增广化结果为 <span class="math inline">\(\boldsymbol{x}_{11}=(0,0,1)\)</span></p>
</blockquote>
<p>增广化后的线性判别函数，判别面是<strong>过原点的超平面</strong>。</p>
<blockquote>
<p>对于过原点的一个判决边界，简化了我们计算权向量的过程。</p>
</blockquote>
<p>同时对于二类问题，有</p>
<p>若 <span class="math inline">\(g(\boldsymbol{y})=\boldsymbol{a}^T\boldsymbol{y}&gt;0\)</span>，则
<span class="math inline">\(\boldsymbol{y}\in\omega_1\)</span>，即 <span class="math inline">\(\boldsymbol{x}\in\omega_1\)</span>；</p>
<p>若 <span class="math inline">\(g(\boldsymbol{y})=\boldsymbol{a}^T\boldsymbol{y}&lt;0\)</span>，则
<span class="math inline">\(\boldsymbol{y}\in\omega_2\)</span>，即 <span class="math inline">\(\boldsymbol{x}\in\omega_2\)</span>。</p>
<figure>
<img src="/linear-discriminant-function/一个三维增广化后的特征空间.png" alt="一个三维增广化后的特征空间">
<figcaption aria-hidden="true">一个三维增广化后的特征空间</figcaption>
</figure>
<h3 id="规范化">规范化</h3>
<p>沿着上面的增广化，比如说对于一个一维二类线性判别函数，它增广化后的判决面（二维空间）是权空间中的一条直线，这条直线将不同类别的向量区分开。</p>
<figure>
<img src="/linear-discriminant-function/非规范化样本.png" alt="非规范化样本">
<figcaption aria-hidden="true">非规范化样本</figcaption>
</figure>
<p>于是乎我们可以得到在某个范围内的<strong>权向量</strong> <span class="math inline">\(\boldsymbol{a}\)</span>
构造出的直线都能满足这个条件，我们称这个区域叫做<strong>解区域</strong>（solution
region）。</p>
<p>但在计算上，验证某些向量是否被这条直线错误的区别是繁琐的，但学习过程中却又需要大量验证某些向量是否被错误分类了，故我们需要对这些向量进行<strong>规范化</strong>。</p>
<p>我们不妨令对于所有 <span class="math inline">\(\boldsymbol{y}\in
\omega_2\)</span> 都有 <span class="math display">\[
\boldsymbol{y}=-\boldsymbol{y}
\]</span> 这样，我们就使得所有的正确分类都有 <span class="math display">\[
g(\boldsymbol{y})&gt;0
\]</span> <img src="/linear-discriminant-function/规范化样本.png" alt="规范化样本"></p>
<p>当我们需要判断某个向量是否被权向量错误分类时，仅需有如下规则：</p>
<p>若 <span class="math inline">\(g(\boldsymbol{y})&gt;0\)</span>，则
<span class="math inline">\(\boldsymbol{y}\)</span>
被<strong>正确</strong>分类；</p>
<p>若 <span class="math inline">\(g(\boldsymbol{y})&lt;0\)</span>，则
<span class="math inline">\(\boldsymbol{y}\)</span>
被<strong>错误</strong>分类。</p>
<p>这极大的简化了运算过程。</p>
<p>跟随上文，我们有如下被增广化后的向量 <span class="math display">\[
\omega_1:\boldsymbol{x}_{11}=(1,0,0),\boldsymbol{x}_{12}=(1,0,1)\\
\omega_2:\boldsymbol{x}_{21}=(1,1,0),\boldsymbol{x}_{22}=(1,1,1)\\
\]</span> 对其进行规范化，有 <span class="math display">\[
\begin{align}
&amp;\omega_1:\boldsymbol{x}_{11}=(1,0,0),\boldsymbol{x}_{12}=(1,0,1)\\
&amp;\omega_2:\boldsymbol{x}_{21}=(-1,-1,0),\boldsymbol{x}_{22}=(-1,-1,-1)
\end{align}
\]</span> 对于满足所有上述条件的向量 <span class="math inline">\(\boldsymbol{a}\)</span>
都称作是<strong>解向量</strong>，也可以发现，每个学习样本都对解向量做了限制，但解向量也不唯一。</p>
<p>如果存在解向量 <span class="math inline">\(\boldsymbol{a}\)</span>
使得二类样本分类正确，则样本被称作是<strong>线性可分</strong>的；反之称作<strong>线性不可分</strong>。</p>
<figure>
<img src="/linear-discriminant-function/线性可分.png" alt="线性可分">
<figcaption aria-hidden="true">线性可分</figcaption>
</figure>
<h3 id="学习算法原理">学习算法原理</h3>
<p>要求解解向量 <span class="math inline">\(\boldsymbol{a}\)</span>，根据上面的理论我们需要通过学习样本来求解不等式组
<span class="math display">\[
\boldsymbol{a}^T\boldsymbol{y}_i&gt;0,i=1,2,\cdots,n
\]</span>
但当权向量维度很大，学习样本足够多时，显然<strong>直接求解不等式组是困难的</strong>。</p>
<p>同时我们希望在样本<strong>线性可分</strong>时，得到的判别函数能够将所有的训练样本正确分类；在样本<strong>线性不可分</strong>时，得到的判别函数产生错误的概率最小（或者称误差最小）。</p>
<p>为转化困难问题为简单问题，我们定义一个<strong>标量函数</strong> <span class="math inline">\(J(\boldsymbol{a})\)</span>，<strong>如果 <span class="math inline">\(J(\boldsymbol{a})\)</span>
的值越小，则判别面的分割质量越高</strong>。</p>
<blockquote>
<p>标量函数指的是输出值为标量的函数，同时有时也称准则函数。</p>
</blockquote>
<p>那么求解解向量 <span class="math inline">\(\boldsymbol{a}\)</span>
就变为求解标量函数 <span class="math inline">\(J(\boldsymbol{a})\)</span>
极小值（最小值）的问题。</p>
<p>那么如何定义合理的标量函数 <span class="math inline">\(J(\boldsymbol{a})\)</span>
能更准确的表示它的含义成为了新问题。</p>
<p>最直观的准则函数的定义是<strong>最少错分样本数准则</strong>，即令
<span class="math inline">\(J(\boldsymbol{a})\)</span>
为样本集合中被错误分类的样本数。</p>
<figure>
<img src="/linear-discriminant-function/最少错分样本数准则下的准则函数.png" alt="最少错分样本数准则下的准则函数">
<figcaption aria-hidden="true">最少错分样本数准则下的准则函数</figcaption>
</figure>
<p>但我们可以直观的发现，此时 <span class="math inline">\(J(\boldsymbol{a})\)</span>
是一个分段函数，对于这种函数在性质上是不被喜欢的，因为它<strong>不光滑</strong>，无法用现有的高效的数学工具求解函数极小值，对计算机来说可能将要遍历所有的函数值才能找到它的极小值。</p>
<p>因此，一个更好的选择是<strong>错分样本到判别面的距离之和准则</strong>，也称作<strong>感知器准则</strong>，<strong>感知器准则函数</strong>的定义如下
<span class="math display">\[
J_p(\boldsymbol{a})=\sum_{\boldsymbol{y}\in
Y_e}(-\boldsymbol{a}^T\boldsymbol{y})
\]</span> 即选择了某个解向量 <span class="math inline">\(\boldsymbol{a}\)</span>
后，被错分的样本到判别面的距离之和，其中 <span class="math inline">\(Y_e\)</span> 代表所有被错分向量的集合。</p>
<blockquote>
<p>这里的计算是规范化后的计算，所以需要加个负号，这样仅需关注距离和。</p>
</blockquote>
<p>此时满足 <span class="math inline">\(J_p(\boldsymbol{a})\geq
0\)</span>，其存在极小值使得样本无错分或错分误差最小。</p>
<blockquote>
<p>当且仅当样本线性可分时，极小值取得 0。</p>
</blockquote>
<h3 id="梯度下降算法">梯度下降算法</h3>
<p>梯度下降算法是一个用来求函数极小值的算法，仅需要知道函数在各点的导数，便可以快速迭代找到极小值点。</p>
<blockquote>
<p>需要注意的是，梯度下降算法仅适用于局部连续可导函数。</p>
</blockquote>
<h4 id="原理">原理</h4>
<p>对于某一个局部连续可导函数 <span class="math inline">\(f(x)\)</span>。</p>
<ol type="1">
<li>（随机）选取一个初始值 <span class="math inline">\(x_0\)</span>；</li>
<li>计算 <span class="math inline">\(f&#39;(x_0)\)</span>，如果值为负则增加 <span class="math inline">\(x_0\)</span>；如果值为正则减小 <span class="math inline">\(x_0\)</span>；</li>
<li>使 <span class="math inline">\(x_1=x_0-\eta
f&#39;(x_0)\)</span>，代入 <span class="math inline">\(x_1\)</span>
重复第二步，直到 <span class="math inline">\(|\eta
f&#39;(x_{n-1})|&lt;\theta\)</span>，<span class="math inline">\(\theta\)</span> 为临界值（误差值）。</li>
</ol>
<p>我们也称 <span class="math inline">\(\eta\)</span>
为<strong>学习率</strong>。</p>
<p>如果对于多参函数，例如二元函数 <span class="math inline">\(f(x,y)\)</span>，我们选择以某一点的梯度来增长或减小。</p>
<blockquote>
<p>因为我们是想让值快速迭代到极小值点处，而梯度是多元函数增长速度最快的方向，则负梯度是多元函数下降速度最快的方向。需要知道，<span class="math inline">\(\|\nabla J(\boldsymbol{a})\|\)</span> 表示其在
<span class="math inline">\(\boldsymbol{a}\)</span>
处的变化率的大小，当某点梯度为 <span class="math inline">\(\boldsymbol{0}\)</span> 时，该点即为 <span class="math inline">\(J(\boldsymbol{a})\)</span> 的极值点。</p>
</blockquote>
<p>即第三步的式子变为 <span class="math display">\[
x_n=x_{n-1}-\eta\nabla f(x_{n-1}, y_{n-1})\\
y_n=y_{n-1}-\eta\nabla f(x_{n-1}, y_{n-1})
\]</span> 其中 <span class="math display">\[
\nabla f(x_{n-1},y_{n-1})=\begin{bmatrix}
\frac{\partial}{\partial x}f(x_{n-1},y_{n-1})\\
\frac{\partial}{\partial y}f(x_{n-1},y_{n-1})
\end{bmatrix}
\]</span> 同理如果对于 <span class="math inline">\(J(\boldsymbol{a})\)</span>，有 <span class="math display">\[
\nabla J(\boldsymbol{a})=\begin{bmatrix}
\frac{\partial}{\partial a_1}\\
\frac{\partial}{\partial a_2}\\
\vdots\\
\frac{\partial}{\partial a_{d+1}}\\
\end{bmatrix}J(\boldsymbol{a})
\]</span>
由于其通过梯度快速下降，故称作<strong>梯度下降法</strong>。</p>
<h4 id="代码">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x * x + <span class="number">2</span> * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">df</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * x + <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">df, init_val = <span class="number">1</span>, learning_rate = <span class="number">0.01</span>, precision = <span class="number">1e-5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Using Gradient Descent to get local minimum</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    `Parameter`:</span></span><br><span class="line"><span class="string">        df - Derivative</span></span><br><span class="line"><span class="string">        init_val - Initial value</span></span><br><span class="line"><span class="string">        learning_rate - Learning rate</span></span><br><span class="line"><span class="string">        precision - Precision</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    `Return`</span></span><br><span class="line"><span class="string">        Local minimun</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = init_val</span><br><span class="line">    xs = [x]</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        x = x - learning_rate * df(x)</span><br><span class="line">        xs.append(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(learning_rate * df(x)) &lt; precision:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Cost %d times calculation.&quot;</span> % <span class="built_in">len</span>(xs))</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(gradient_descent(df))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Cost 412 times calculation.</span></span><br><span class="line"><span class="string">-0.9995046101486759</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<h4 id="优劣">优劣</h4>
<p>如果 <span class="math inline">\(\eta\)</span>
过大可能会导致<strong>过冲</strong>（overshoot）甚至发散，即在极小值两边迭代无法退出循环；<span class="math inline">\(\eta\)</span>
过小会导致迭代次数过多，浪费算力。所以一般会选择较小但不是特别小的 <span class="math inline">\(\eta\)</span> 进行梯度下降。</p>
<h3 id="牛顿下降法">牛顿下降法</h3>
<p>基于梯度下降算法和牛顿迭代法进行改进，得到牛顿下降法。</p>
<h4 id="原理-1">原理</h4>
<ol type="1">
<li>（随机）选取一个初始值 <span class="math inline">\(\boldsymbol{x}_0\)</span>；</li>
<li>计算 <span class="math inline">\(\nabla f(\boldsymbol{x}_0)\)</span>
和 <span class="math inline">\(\boldsymbol{H}^{-1}\)</span>；</li>
<li>使 <span class="math inline">\(\boldsymbol{x}_1=\boldsymbol{x}_0-\boldsymbol{H}^{-1}\nabla
f(\boldsymbol{x}_0)\)</span>，代入 <span class="math inline">\(x_1\)</span> 重复第二步，直到 <span class="math inline">\(|\boldsymbol{H}^{-1}\nabla
f(\boldsymbol{x}_{n-1})|&lt;\theta\)</span>，<span class="math inline">\(\theta\)</span> 为临界值（误差值）。</li>
</ol>
<p>其中 <span class="math inline">\(H\)</span>
为<strong>海森矩阵</strong>。</p>
<h4 id="优劣-1">优劣</h4>
<p>牛顿下降法可以更快更准确的进行<strong>下降</strong>，但反过来计算
<span class="math inline">\(H\)</span>
的逆矩阵是困难的，所以一般并不会使用牛顿下降法，而是会选择小 <span class="math inline">\(\eta\)</span> 的梯度下降法。</p>
<h3 id="感知器梯度下降法">感知器梯度下降法</h3>
<p>在了解了梯度下降算法后，我们回到判别函数 <span class="math inline">\(g(\boldsymbol{y})\)</span> 的感知器准则函数 <span class="math inline">\(J_p(\boldsymbol{a})\)</span>。</p>
<p>我们由原式可得，<strong>感知器准则函数的梯度</strong>为 <span class="math display">\[
\nabla J_p=\sum_{\boldsymbol{y}\in Y_e}(-\boldsymbol{y})
\]</span> 那么我们就可以把梯度下降的公式改写为 <span class="math display">\[
\boldsymbol{a}_{n+1}=\boldsymbol{a}_{n}+\eta(n)\sum_{\boldsymbol{y}\in
Y_e}\boldsymbol{y}
\]</span> 结束条件直到 <span class="math inline">\(|\eta(n)\sum_{\boldsymbol{y}\in
Y_e}\boldsymbol{y}|&lt;\theta\)</span>。</p>
<h4 id="代码-1">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample</span></span><br><span class="line">omega1 = [(<span class="number">4</span>, <span class="number">28</span>), (<span class="number">7</span>, <span class="number">23</span>), (<span class="number">16</span>, <span class="number">27</span>), (<span class="number">18</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="number">17</span>), (<span class="number">13</span>, <span class="number">24</span>), (<span class="number">2</span>, <span class="number">24</span>), (<span class="number">12</span>, <span class="number">29</span>), (<span class="number">2</span>, <span class="number">21</span>), (<span class="number">5</span>, <span class="number">26</span>), (<span class="number">2</span>, <span class="number">24</span>), (<span class="number">11</span>, <span class="number">18</span>), (<span class="number">8</span>, <span class="number">28</span>), (<span class="number">3</span>, <span class="number">21</span>), (<span class="number">13</span>, <span class="number">22</span>)]</span><br><span class="line">omega2 = [(<span class="number">14</span>, <span class="number">12</span>), (<span class="number">25</span>, <span class="number">28</span>), (<span class="number">8</span>, <span class="number">7</span>), (<span class="number">26</span>, <span class="number">29</span>), (<span class="number">29</span>, <span class="number">23</span>), (<span class="number">18</span>, <span class="number">1</span>), (<span class="number">19</span>, <span class="number">12</span>), (<span class="number">19</span>, <span class="number">25</span>), (<span class="number">23</span>, <span class="number">17</span>), (<span class="number">30</span>, <span class="number">2</span>), (<span class="number">18</span>, <span class="number">22</span>), (<span class="number">28</span>, <span class="number">17</span>), (<span class="number">24</span>, <span class="number">20</span>), (<span class="number">9</span>, <span class="number">4</span>), (<span class="number">25</span>, <span class="number">14</span>)]</span><br><span class="line">omega1 = np.array(omega1)</span><br><span class="line">omega2 = np.array(omega2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">x = omega1[:, <span class="number">0</span>]</span><br><span class="line">y = omega1[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">x = omega2[:, <span class="number">0</span>]</span><br><span class="line">y = omega2[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, c=<span class="string">&#x27;green&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Augmented Vector</span></span><br><span class="line">omega1 = np.hstack((omega1, np.ones((<span class="built_in">len</span>(omega1), <span class="number">1</span>))))</span><br><span class="line">omega2 = np.hstack((omega2, np.ones((<span class="built_in">len</span>(omega1), <span class="number">1</span>))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization</span></span><br><span class="line">omega2 = -omega2</span><br><span class="line">samples = np.vstack([omega1, omega2])</span><br><span class="line">sample_number = <span class="built_in">len</span>(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Laerning Rate</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Criterion Function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Jp</span>(<span class="params">a, ys</span>):</span><br><span class="line">    rst = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> ys:</span><br><span class="line">        <span class="keyword">if</span> np.dot(a, y) &lt; <span class="number">0</span>:</span><br><span class="line">            rst += -np.dot(a, y)</span><br><span class="line">    <span class="keyword">return</span> rst</span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient of Criterion Function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nablaJp</span>(<span class="params">a, ys</span>):</span><br><span class="line">    rst = np.zeros(ys.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> ys:</span><br><span class="line">        <span class="keyword">if</span> np.dot(a, y) &lt; <span class="number">0</span>:</span><br><span class="line">            rst += -y</span><br><span class="line">    <span class="keyword">return</span> rst</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perceptron Gradient Descent</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perceptron</span>(<span class="params">samples, eta, theta = <span class="number">0.001</span></span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    a = np.ones(samples.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        nabla = nablaJp(a, samples)</span><br><span class="line">        <span class="keyword">if</span> np.linalg.norm(eta * nabla) &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        a = a - eta * nabla</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Perceptron cost %d times iterate.&quot;</span> % count)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment"># Solve Weight Vector</span></span><br><span class="line">a = perceptron(samples, eta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn to Feature Space</span></span><br><span class="line">xs = []</span><br><span class="line">ys = []</span><br><span class="line">xs += [<span class="number">0</span>, <span class="number">20</span>]</span><br><span class="line">ys += [(xs[<span class="number">0</span>] * a[<span class="number">0</span>] + a[<span class="number">2</span>]) / -a[<span class="number">1</span>], (xs[<span class="number">1</span>] * a[<span class="number">0</span>] + a[<span class="number">2</span>]) / -a[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot Judgment Boundary</span></span><br><span class="line">plt.plot(xs, ys)</span><br><span class="line"><span class="built_in">print</span>(Jp(a, samples))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Perceptron cost 7 times iterate.</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>对于这组样本，仅需 7 次迭代即求得解向量。</p>
<figure>
<img src="/linear-discriminant-function/二维二类权空间的感知器梯度下降.png" alt="二维二类权空间的感知器梯度下降">
<figcaption aria-hidden="true">二维二类权空间的感知器梯度下降</figcaption>
</figure>
<h3 id="固定增量法">固定增量法</h3>
<p>感知器梯度下降法需要<strong>一次获取所有学习样本</strong>，并在迭代算法中一次遍历所有样本。</p>
<p>但实际应用中，有时样本是<strong>分批获取</strong>的，先训练一部分样本，再训练一部分样本。</p>
<p>固定增量法就是根据以上需求改变的感知器算法，基本思想是每次修改解向量时，不遍历所有样本，而是将学习样本串行输入，<strong>每考察一个样本就对解向量
<span class="math inline">\(a\)</span> 进行一次修改</strong>。</p>
<p>即迭代公式为 <span class="math display">\[
\boldsymbol{a}_{n+1}=\boldsymbol{a}_{n}+\eta(n)\boldsymbol{y},\boldsymbol{y}\in
Y_e
\]</span> 结束条件直到 <span class="math inline">\(|\boldsymbol{a}_{n+1}-\boldsymbol{a}_{n}|&lt;\theta\)</span>。</p>
<p>这样做到了每当解向量分类错误时，就修改解向量，在算法时间复杂度不变的情况下，提升了解向量迭代的效率。</p>
<h4 id="代码-2">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample</span></span><br><span class="line">omega1 = [(<span class="number">4</span>, <span class="number">28</span>), (<span class="number">7</span>, <span class="number">23</span>), (<span class="number">16</span>, <span class="number">27</span>), (<span class="number">18</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="number">17</span>), (<span class="number">13</span>, <span class="number">24</span>), (<span class="number">2</span>, <span class="number">24</span>), (<span class="number">12</span>, <span class="number">29</span>), (<span class="number">2</span>, <span class="number">21</span>), (<span class="number">5</span>, <span class="number">26</span>), (<span class="number">2</span>, <span class="number">24</span>), (<span class="number">11</span>, <span class="number">18</span>), (<span class="number">8</span>, <span class="number">28</span>), (<span class="number">3</span>, <span class="number">21</span>), (<span class="number">13</span>, <span class="number">22</span>)]</span><br><span class="line">omega2 = [(<span class="number">14</span>, <span class="number">12</span>), (<span class="number">25</span>, <span class="number">28</span>), (<span class="number">8</span>, <span class="number">7</span>), (<span class="number">26</span>, <span class="number">29</span>), (<span class="number">29</span>, <span class="number">23</span>), (<span class="number">18</span>, <span class="number">1</span>), (<span class="number">19</span>, <span class="number">12</span>), (<span class="number">19</span>, <span class="number">25</span>), (<span class="number">23</span>, <span class="number">17</span>), (<span class="number">30</span>, <span class="number">2</span>), (<span class="number">18</span>, <span class="number">22</span>), (<span class="number">28</span>, <span class="number">17</span>), (<span class="number">24</span>, <span class="number">20</span>), (<span class="number">9</span>, <span class="number">4</span>), (<span class="number">25</span>, <span class="number">14</span>)]</span><br><span class="line">omega1 = np.array(omega1)</span><br><span class="line">omega2 = np.array(omega2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">x = omega1[:, <span class="number">0</span>]</span><br><span class="line">y = omega1[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">x = omega2[:, <span class="number">0</span>]</span><br><span class="line">y = omega2[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, c=<span class="string">&#x27;green&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Augmented Vector</span></span><br><span class="line">omega1 = np.hstack((omega1, np.ones((<span class="built_in">len</span>(omega1), <span class="number">1</span>))))</span><br><span class="line">omega2 = np.hstack((omega2, np.ones((<span class="built_in">len</span>(omega1), <span class="number">1</span>))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization</span></span><br><span class="line">omega2 = -omega2</span><br><span class="line">samples = np.vstack([omega1, omega2])</span><br><span class="line">sample_number = <span class="built_in">len</span>(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Laerning Rate</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Criterion Function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Jp</span>(<span class="params">a, ys</span>):</span><br><span class="line">    rst = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> ys:</span><br><span class="line">        <span class="keyword">if</span> np.dot(a, y) &lt; <span class="number">0</span>:</span><br><span class="line">            rst += -np.dot(a, y)</span><br><span class="line">    <span class="keyword">return</span> rst</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perceptron Gradient Descent</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perceptron</span>(<span class="params">samples, eta, theta = <span class="number">0.001</span></span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    a = np.ones(samples.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        ap = a</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> samples:</span><br><span class="line">            <span class="keyword">if</span> np.dot(a, y) &lt; <span class="number">0</span>:</span><br><span class="line">                a = a + eta * y</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> np.linalg.norm(a - ap) &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Perceptron cost %d times iterate.&quot;</span> % count)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment"># Solve Weight Vector</span></span><br><span class="line">a = perceptron(samples, eta)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn to Feature Space</span></span><br><span class="line">xs = []</span><br><span class="line">ys = []</span><br><span class="line">xs += [<span class="number">0</span>, <span class="number">20</span>]</span><br><span class="line">ys += [(xs[<span class="number">0</span>] * a[<span class="number">0</span>] + a[<span class="number">2</span>]) / -a[<span class="number">1</span>], (xs[<span class="number">1</span>] * a[<span class="number">0</span>] + a[<span class="number">2</span>]) / -a[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot Judgment Boundary</span></span><br><span class="line">plt.plot(xs, ys)</span><br><span class="line"><span class="built_in">print</span>(Jp(a, samples))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Perceptron cost 5 times iterate.</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>对于这组样本，仅需 5 次迭代即求得解向量。</p>
<figure>
<img src="/linear-discriminant-function/二维二类权空间的固定增量法.png" alt="二维二类权空间的固定增量法">
<figcaption aria-hidden="true">二维二类权空间的固定增量法</figcaption>
</figure>
<p>当样本数量增多时，固定增量法的效率优化会更明显。</p>
<p>但固定增量法需要样本满足二类可分条件，否则无法退出迭代。</p>
<p><strong>感知器算法收敛定理：如果训练样本是线性可分的，固定增量法给出的权向量序列必定终止于某个解向量。</strong></p>
<h3 id="变增量梯度算法">变增量梯度算法</h3>
<p>单一的 <span class="math inline">\(\eta\)</span>
会导致效率一定，这个时候就会想使得 <span class="math inline">\(\eta\)</span> 会随着迭代次数的增加而变化，即 <span class="math display">\[
\eta\to\eta(n)
\]</span> 我们可以证明得到，当对于所有 <span class="math inline">\(n\)</span> 有 <span class="math display">\[
\eta(n)\geq 0\\
\lim_{m\to\infty}\sum^m_{n=1}\eta(n)=\infty\\
\lim_{m\to\infty}\frac{\sum^m_{n=1}\eta^2(n)}{(\sum^m_{n=1}\eta(n))^2}=0
\]</span> 即正项级数 <span class="math inline">\(\eta(n)\)</span>
发散，但满足一定条件，那么我们可以证明其迭代算法 <span class="math display">\[
\boldsymbol{a}_{n+1}=\boldsymbol{a}_{n}+\eta(n)\boldsymbol{y}
\]</span> 是一定收敛的。</p>
<blockquote>
<p>这也就是为什么在之前的公式中的学习率都以 <span class="math inline">\(\eta(n)\)</span> 表示。</p>
</blockquote>
<h3 id="带裕量的感知器算法">带裕量的感知器算法</h3>
<p>我们知道，解向量可以有无数多种，解区域也是有边界的，有的时候我们并不想让结果解向量太靠近边界，这样会使得分类严格。</p>
<p>根据固定增量法进行改进，我们加入裕量（margin）来均衡解向量的结果，使得迭代算法变为：</p>
<p>当 <span class="math inline">\(\boldsymbol{a}^T\boldsymbol{y}_i\leq
b\)</span> 时，我们就修改解向量 <span class="math display">\[
\boldsymbol{a}_{n+1}=\boldsymbol{a}_{n}+\eta(n)\boldsymbol{y}
\]</span></p>
<h4 id="代码-3">代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample</span></span><br><span class="line">omega1 = [(<span class="number">4</span>, <span class="number">28</span>), (<span class="number">7</span>, <span class="number">23</span>), (<span class="number">16</span>, <span class="number">27</span>), (<span class="number">18</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="number">17</span>), (<span class="number">13</span>, <span class="number">24</span>), (<span class="number">2</span>, <span class="number">24</span>), (<span class="number">12</span>, <span class="number">29</span>), (<span class="number">2</span>, <span class="number">21</span>), (<span class="number">5</span>, <span class="number">26</span>), (<span class="number">2</span>, <span class="number">24</span>), (<span class="number">11</span>, <span class="number">18</span>), (<span class="number">8</span>, <span class="number">28</span>), (<span class="number">3</span>, <span class="number">21</span>), (<span class="number">13</span>, <span class="number">22</span>)]</span><br><span class="line">omega2 = [(<span class="number">14</span>, <span class="number">12</span>), (<span class="number">25</span>, <span class="number">28</span>), (<span class="number">8</span>, <span class="number">7</span>), (<span class="number">26</span>, <span class="number">29</span>), (<span class="number">29</span>, <span class="number">23</span>), (<span class="number">18</span>, <span class="number">1</span>), (<span class="number">19</span>, <span class="number">12</span>), (<span class="number">19</span>, <span class="number">25</span>), (<span class="number">23</span>, <span class="number">17</span>), (<span class="number">30</span>, <span class="number">2</span>), (<span class="number">18</span>, <span class="number">22</span>), (<span class="number">28</span>, <span class="number">17</span>), (<span class="number">24</span>, <span class="number">20</span>), (<span class="number">9</span>, <span class="number">4</span>), (<span class="number">25</span>, <span class="number">14</span>)]</span><br><span class="line">omega1 = np.array(omega1)</span><br><span class="line">omega2 = np.array(omega2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot</span></span><br><span class="line">x = omega1[:, <span class="number">0</span>]</span><br><span class="line">y = omega1[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, c=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line">x = omega2[:, <span class="number">0</span>]</span><br><span class="line">y = omega2[:, <span class="number">1</span>]</span><br><span class="line">plt.scatter(x, y, s=<span class="number">20</span>, c=<span class="string">&#x27;green&#x27;</span>, marker=<span class="string">&#x27;s&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Augmented Vector</span></span><br><span class="line">omega1 = np.hstack((omega1, np.ones((<span class="built_in">len</span>(omega1), <span class="number">1</span>))))</span><br><span class="line">omega2 = np.hstack((omega2, np.ones((<span class="built_in">len</span>(omega1), <span class="number">1</span>))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalization</span></span><br><span class="line">omega2 = -omega2</span><br><span class="line">samples = np.vstack([omega1, omega2])</span><br><span class="line">sample_number = <span class="built_in">len</span>(samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Laerning Rate</span></span><br><span class="line">eta = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Criterion Function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Jp</span>(<span class="params">a, ys</span>):</span><br><span class="line">    rst = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> ys:</span><br><span class="line">        <span class="keyword">if</span> np.dot(a, y) &lt; <span class="number">0</span>:</span><br><span class="line">            rst += -np.dot(a, y)</span><br><span class="line">    <span class="keyword">return</span> rst</span><br><span class="line"></span><br><span class="line"><span class="comment"># Perceptron Gradient Descent</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perceptron</span>(<span class="params">samples, eta, theta = <span class="number">0.001</span>, b = <span class="number">0</span></span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    a = np.ones(samples.shape[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        ap = a</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> samples:</span><br><span class="line">            <span class="keyword">if</span> np.dot(a, y) &lt; b:</span><br><span class="line">                a = a + eta * y</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> np.linalg.norm(a - ap) &lt; theta:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Perceptron cost %d times iterate.&quot;</span> % count)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="comment"># Solve Weight Vector</span></span><br><span class="line">a = perceptron(samples, eta, b = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn to Feature Space</span></span><br><span class="line">xs = []</span><br><span class="line">ys = []</span><br><span class="line">xs += [<span class="number">0</span>, <span class="number">20</span>]</span><br><span class="line">ys += [(xs[<span class="number">0</span>] * a[<span class="number">0</span>] + a[<span class="number">2</span>]) / -a[<span class="number">1</span>], (xs[<span class="number">1</span>] * a[<span class="number">0</span>] + a[<span class="number">2</span>]) / -a[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot Judgment Boundary</span></span><br><span class="line">plt.plot(xs, ys)</span><br><span class="line"><span class="built_in">print</span>(Jp(a, samples))</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Perceptron cost 7 times iterate.</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<p>对于这组样本，需 7 次迭代求得解向量。</p>
<p>不难看出，带裕量的感知器算法会使得迭代次数变多，但这是可以控制的，远低于原始的感知器梯度下降法的迭代次数。</p>
<p>同时我们通过添加裕量 <span class="math inline">\(b=1\)</span>，使得判决边界更加趋近于解空间的中间。</p>
<figure>
<img src="/linear-discriminant-function/二维二类权空间的带裕量的固定增量法.png" alt="二维二类权空间的带裕量的固定增量法">
<figcaption aria-hidden="true">二维二类权空间的带裕量的固定增量法</figcaption>
</figure>
<h2 id="fisher-线性判别">Fisher 线性判别</h2>
<h3 id="原理-2">原理</h3>
<p>我们知道上面线性判别的方式是增加维度的，这样可以精确的描述一个线性判别函数；同时特征空间的纬度越高，判别的精确度就越高。</p>
<p>但这与此同时也带来一个灾难性的问题，就是高纬度的计算总是复杂和苦难的，因此
Fisher 研究的是降维领域。</p>
<p>由判别函数的式子可知 <span class="math display">\[
g(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}
\]</span>
可以理解为特征向量在权向量上的投影，如果说我们对所有的特征向量都做一次投影，应该是下图所示</p>
<figure>
<img src="/linear-discriminant-function/各向量对权向量的投影.png" alt="各向量对权向量的投影">
<figcaption aria-hidden="true">各向量对权向量的投影</figcaption>
</figure>
<p>可以很自然的发现，我们将二维的权空间转化到了一维上，此时我们只考虑投影值的大小，而非特征向量的空间分布。</p>
<p>如果说把其转化为一维可视图像，正如图所示</p>
<figure>
<img src="/linear-discriminant-function/Fisher准则一维化.png" alt="Fisher准则一维化">
<figcaption aria-hidden="true">Fisher准则一维化</figcaption>
</figure>
<p>很显然，我们发现不同的权向量会导致一维图像上有不同的结果，左图的权向量显然错分数更多，映射到一维图像上就是有所重合；有图的权向量显然错分数更少，映射到一维图像上就是类间数据有所分离。</p>
<p>自然的，我们会认为，最好的分类是将类间数据进行分离，而类内数据进行聚合，使得两类分离，这就是
Fisher 判别准则。</p>
<p><strong>LDA
基本原理：投影后具有最佳可分离性，即最大的类间距离和最小的类内距离。</strong></p>
<p>我们定义类内距离以投影均值表示，类间距离以投影方差表示，那么想要同时优化两个目标，即想让样本均值之差大，样本方差尽量小，那么定义准则函数
<span class="math display">\[
J(\boldsymbol{w})=\frac{|\tilde{m}_1-\tilde{m}_2|}{\tilde{s}_1^2+\tilde{s}_2^2}
\]</span> 那么有样本均值 <span class="math display">\[
\boldsymbol{m}_i=\frac{1}{n_i}\sum\boldsymbol{x}
\]</span> 推导可得 <span class="math display">\[
\begin{align}
\tilde{m}_i&amp;=\frac{1}{n_i}\sum g(\boldsymbol{x})\\
&amp;=g(\boldsymbol{m}_i)
\end{align}
\]</span> 同时 <span class="math display">\[
\begin{align}
\tilde{s}_i^2&amp;=\sum(g(\boldsymbol{x})-\tilde{m}_i)\\
&amp;=\sum\boldsymbol{w}^T(\boldsymbol{x}-\boldsymbol{m}_i)(\boldsymbol{x}-\boldsymbol{m}_i)^T\boldsymbol{w}\\
&amp;=\boldsymbol{w}^T\boldsymbol{S}_i\boldsymbol{w}
\end{align}
\]</span> 那么总的就有 <span class="math display">\[
\begin{align}
\tilde{s}_1^2+\tilde{s}_2^2&amp;=\boldsymbol{w}^T(\boldsymbol{S}_1+\boldsymbol{S}_2)\boldsymbol{w}\\
&amp;=\boldsymbol{w}^T\boldsymbol{S}_w\boldsymbol{w}\\
(\tilde{m}_1-\tilde{m}_2)^2&amp;=\boldsymbol{w}^T(\boldsymbol{m}_1-\boldsymbol{m}_2)(\boldsymbol{m}_1-\boldsymbol{m}_2)^T\boldsymbol{w}\\
&amp;=\boldsymbol{w}^T\boldsymbol{S}_B\boldsymbol{w}
\end{align}
\]</span> 那么准则函数变为 <span class="math display">\[
J(\boldsymbol{w})=\frac{\boldsymbol{w}^T\boldsymbol{S}_B\boldsymbol{w}}{\boldsymbol{w}^T\boldsymbol{S}_W\boldsymbol{w}}
\]</span>
我们想要找到的是这个准则函数的最大值，那么此时需要找到该函数的导数为零点。</p>
<p>可以最终计算得到 <span class="math display">\[
J&#39;(\boldsymbol{w})=0\Rightarrow
\boldsymbol{w}=\boldsymbol{S}_W^{-1}(\boldsymbol{m}_1-\boldsymbol{m}_2)
\]</span></p>
<h3 id="示例">示例</h3>
<p>现有某商品评分表如下</p>
<figure>
<img src="/linear-discriminant-function/fisher准则示例图标.png" alt="fisher准则示例图标">
<figcaption aria-hidden="true">fisher准则示例图标</figcaption>
</figure>
<p>现在有评分为 <span class="math inline">\(\boldsymbol{x}^T=(9, 5,
4)\)</span>，判断其是否愿意购买此商品。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample</span></span><br><span class="line">omega1 = [(<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>), (<span class="number">7</span>, <span class="number">6</span>, <span class="number">6</span>), (<span class="number">10</span>, <span class="number">7</span>, <span class="number">8</span>), (<span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>), (<span class="number">9</span>, <span class="number">9</span>, <span class="number">3</span>), (<span class="number">8</span>, <span class="number">6</span>, <span class="number">7</span>), (<span class="number">7</span>, <span class="number">5</span>, <span class="number">6</span>)]</span><br><span class="line">omega2 = [(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>), (<span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>), (<span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>), (<span class="number">6</span>, <span class="number">4</span>, <span class="number">5</span>), (<span class="number">8</span>, <span class="number">2</span>, <span class="number">2</span>)]</span><br><span class="line">omega1 = np.array(omega1)</span><br><span class="line">omega2 = np.array(omega2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get Average</span></span><br><span class="line">m1 = np.average(omega1, axis=<span class="number">0</span>)</span><br><span class="line">m2 = np.average(omega2, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate S_i</span></span><br><span class="line">A = omega1 - m1</span><br><span class="line">B = omega2 - m2</span><br><span class="line"></span><br><span class="line">S1 = np.dot(A.T, A)</span><br><span class="line">S2 = np.dot(B.T, B)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate S_W</span></span><br><span class="line">S = S1 + S2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Weight Vector</span></span><br><span class="line">w = np.dot(np.linalg.inv(S), m1 - m2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate Standard Value</span></span><br><span class="line">b = (np.dot(w, m1) * <span class="built_in">len</span>(omega1) + np.dot(w, m2) * <span class="built_in">len</span>(omega2)) / (<span class="built_in">len</span>(omega1) + <span class="built_in">len</span>(omega2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feature Vector</span></span><br><span class="line">x = (<span class="number">9</span>, <span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> np.dot(w, x) &gt; b:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;omega 1.&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;omega 2.&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>实际上，判决用的判决边界值是该判别函数的标准截距。</p>
</blockquote>
<h2 id="支持向量机">支持向量机</h2>
<h3 id="支持向量">支持向量</h3>
<p>我们知道将训练样本分开的超平面可能有许多种，但是哪一个超平面是好的判决面呢？</p>
<p><img src="/linear-discriminant-function/分类样本的多种超平面.png" alt="分类样本的多种超平面" style="zoom:50%;"></p>
<p>很自然的我们会选择正中间的超平面，因为它的<strong>容忍性好，鲁棒性高，泛化能力最强</strong>。换句话说，它拥有的<strong>裕量大</strong>。</p>
<p>于是我们定义<strong>最优分类界面</strong>是<strong>给定线性可分样本集，能够将样本分开的最大间隔超平面</strong>。</p>
<p>其中，样本集与分类界面之间的距离 <span class="math inline">\(r\)</span>
定义为<strong>样本与分类界面之间几何间隔的最小值</strong>。</p>
<figure>
<img src="/linear-discriminant-function/支撑向量示意图.png" alt="支撑向量示意图">
<figcaption aria-hidden="true">支撑向量示意图</figcaption>
</figure>
<p>我们把距离最优分类界面最近的这些训练样本称作<strong>支持向量</strong>，那么可以发现最优分类界面完全由支持矢量决定。</p>
<blockquote>
<p>支持向量就像是维持超平面稳定的柱子。</p>
</blockquote>
<p>然而支持向量的寻找比较困难。</p>
<p>我们将其转化，由判决函数 <span class="math display">\[
g(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b
\]</span> 知道解向量可以任意伸缩，同时保持超平面不变，即 <span class="math inline">\(g(\boldsymbol{x})=0\)</span>。</p>
<p>现在缩放 <span class="math inline">\(\boldsymbol{w}\)</span> 和 <span class="math inline">\(b\)</span> 使得对 <span class="math inline">\(\omega_1\)</span> 类的支持向量满足 <span class="math inline">\(g(\boldsymbol{x})=1\)</span> 而对 <span class="math inline">\(\omega_2\)</span> 类的支持向量满足 <span class="math inline">\(g(\boldsymbol{x})=-1\)</span>。</p>
<p>那么可以用方程表示为 <span class="math display">\[
\boldsymbol{w}^T\boldsymbol{x}_i+b\geq +1\\
\boldsymbol{w}^T\boldsymbol{x}_i+b\leq -1
\]</span> 即 <span class="math display">\[
z_kg(\boldsymbol{x}_k)\geq1
\]</span> 模式属于 <span class="math inline">\(\omega_1\)</span> 或是
<span class="math inline">\(\omega_2\)</span> 我们分别令 <span class="math inline">\(z_k=\pm1\)</span>。</p>
<p>由判决面性质可知，距离 <span class="math inline">\(r\)</span> 满足
<span class="math display">\[
r=\frac{g(\boldsymbol{x})}{\|\boldsymbol{w}\|}
\]</span> 定义间隔 <span class="math inline">\(\gamma=\frac{2}{\|\boldsymbol{w}\|}\)</span>，其中可以知道支持向量上满足
<span class="math inline">\(r=\pm1\)</span>。</p>
<figure>
<img src="/linear-discriminant-function/判决函数的支持向量性质.png" alt="判决函数的支持向量性质">
<figcaption aria-hidden="true">判决函数的支持向量性质</figcaption>
</figure>
<p>那么我们更进一步就是想使得间隔 <span class="math inline">\(\gamma\)</span> 最大，不妨定义准则函数 <span class="math display">\[
J(\boldsymbol{w})=\min_a\frac{1}{2}\|\boldsymbol{w}\|^2
\]</span> 同时要满足 <span class="math display">\[
z_kg(\boldsymbol{x})\geq 1
\]</span>
但我们想要找到该准则函数的极小值，由于其带有约束条件，不能使用简单直接的梯度下降法。</p>
<h3 id="不等式约束的极小值">不等式约束的极小值</h3>
<h4 id="朗格朗日定理">朗格朗日定理</h4>
<p><strong>拉格朗日乘数法</strong>指出，要找到 <span class="math inline">\(z=f(x,y)\)</span> 在约束条件 <span class="math inline">\(\varphi(x,y)=0\)</span>
下的极值点，先构造拉格朗日函数 <span class="math display">\[
L(x,y,\lambda)=f(x,y)+\lambda \varphi(x,y)
\]</span> 其中 <span class="math inline">\(\lambda\)</span>
为某一常数。由 <span class="math display">\[
\begin{cases}
f_x(x,y)+\lambda\varphi_x(x,y)=0\\
f_y(x,y)+\lambda\varphi_y(x,y)=0\\
\varphi(x,y)=0
\end{cases}
\]</span> 解出 <span class="math inline">\(x,y,\lambda\)</span>，其中
<span class="math inline">\(x,y\)</span> 就是可能的极值点坐标。</p>
<p>将拉格朗日定理转化为与我们问题相关的形式，问题定义为 <span class="math display">\[
\min f(\boldsymbol{x})\\
\text{s.t. }h_i(\boldsymbol{x})=0,i=1,2,\cdots,l
\]</span> 作出拉格朗日函数 <span class="math display">\[
L(\boldsymbol{x},\boldsymbol{\lambda})=f(\boldsymbol{x})-\sum^{l}_{i=1}\lambda_ih_i(\boldsymbol{x})
\]</span> 其中，<span class="math inline">\(\boldsymbol{\lambda}=(\lambda_1,\lambda_2,\cdots,\lambda_l)^T\)</span>
称作<strong>乘子向量</strong>。</p>
<p>若 <span class="math inline">\(\boldsymbol{x}^*\)</span>
是局部极小值，那么一定存在 <span class="math inline">\(\boldsymbol{\lambda}^*\)</span> 满足 <span class="math display">\[
\nabla_xL(\boldsymbol{x}^*,\boldsymbol{\lambda}^*)=0
\]</span> 即 <span class="math display">\[
\nabla_xf(\boldsymbol{x}^*)-\sum^{l}_{i=1}\lambda^*_i\nabla
h_i(\boldsymbol{x}^*)=0
\]</span></p>
<h4 id="kt-最优性条件">KT 最优性条件</h4>
<p>约束优化的一阶必要条件是由 Kuhn 和 Tuchker 于 1951
年提出，所以一阶必要条件又称为 KT 条件，满足 KT 条件的点又称作 KT
点。</p>
<p>KT 最优性条件是用于求解不等式约束的，问题定义如下 <span class="math display">\[
\min f(\boldsymbol{x})\\
\text{s.t. }h_i(\boldsymbol{x})\geq0,i=1,2,\cdots,l
\]</span> 在拉格朗日定理的基础上，增加约束条件，即为 <span class="math display">\[
\begin{cases}
\nabla_xf(\boldsymbol{x}^*)-\sum^{l}_{i=1}\lambda^*_i\nabla
h_i(\boldsymbol{x}^*)=0\\
h_i(\boldsymbol{x}^*)\geq0,\lambda_i^*\geq0,\lambda^*_ih_i(\boldsymbol{x}^*)=0,i=1,2,\cdots,l
\end{cases}
\]</span> 这样就可以解决带不等式约束的极小值。</p>
<h3 id="对偶问题">对偶问题</h3>
<p>对偶问题是最优化问题的一个重要概念，它与原始问题相对应，通过对偶问题的解，可以推导出原始问题的解或相关信息。</p>
<p>对偶问题的主要思想是：将原问题中的部分约束条件转移到目标函数中，变为目标函数中的惩罚项。从而得到一个新的最优化问题，这就是对偶问题。</p>
<p>比如说一个原始问题是 <span class="math display">\[
\min f(x)\\
\text{s.t. }c(x)=0,h(x)&lt;=0
\]</span> 其中 <span class="math inline">\(f(x)\)</span>
是目标函数，<span class="math inline">\(c(x)=0\)</span>
是等式约束，<span class="math inline">\(h(x)\leq 0\)</span>
是不等式约束。</p>
<p>那么对应的对偶问题是 <span class="math display">\[
\max g(u,v)=uc(x)+vh(x)\\
\text{s.t. }u\geq0,v\geq0
\]</span></p>
<h3 id="支持向量机-1">支持向量机</h3>
<p>依据上面的原理，我们可以得到支持向量机的模型可以转化为 <span class="math display">\[
\begin{cases}
\nabla\frac{1}{2}\|\boldsymbol{w}\|^2-\sum^{l}_{i=1}\lambda_i\nabla(z_kg(x_i)-1)
=0\\
z_kg(x_i)-1\geq 0,\lambda_i\geq 0,\lambda_i g(x_i)=0
\end{cases}
\]</span> 一个两类分类问题有以下点 <span class="math display">\[
\omega_1:(1,1),(1,-1)\\
\omega_2:(-1,1),(-1,-1)
\]</span> 通过以上知识，可以写出代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sympy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Samples</span></span><br><span class="line">omega1 = [(<span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, -<span class="number">1</span>)]</span><br><span class="line">omega2 = [(-<span class="number">1</span>, <span class="number">1</span>), (-<span class="number">1</span>, -<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">samples = omega1 + omega2</span><br><span class="line">sample_number = <span class="built_in">len</span>(omega1) + <span class="built_in">len</span>(omega2)</span><br><span class="line">dimension = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initial</span></span><br><span class="line">w0 = sympy.symbols(<span class="string">&#x27;w0&#x27;</span>)</span><br><span class="line">w = sympy.symbols(<span class="string">f&#x27;w1:<span class="subst">&#123;dimension+<span class="number">1</span>&#125;</span>&#x27;</span>)</span><br><span class="line">Lambda = sympy.symbols(<span class="string">f&#x27;\lambda_1:<span class="subst">&#123;sample_number+<span class="number">1</span>&#125;</span>&#x27;</span>, nonnegative=<span class="literal">True</span>)</span><br><span class="line">z = [<span class="number">1</span>] * <span class="built_in">len</span>(omega1) + [-<span class="number">1</span>] * <span class="built_in">len</span>(omega2)</span><br><span class="line">L = np.dot(w, w) / <span class="number">2</span></span><br><span class="line"><span class="comment"># Relaxtion Conditions</span></span><br><span class="line">relaxtion_conditions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(sample_number):</span><br><span class="line">    relaxtion_conditions.append(Lambda[i] * (z[i] * (np.dot(w, samples[i]) + w0) - <span class="number">1</span>))</span><br><span class="line">    L -= relaxtion_conditions[-<span class="number">1</span>]</span><br><span class="line"><span class="comment"># KT Conditions</span></span><br><span class="line">KT_conditions = [sympy.diff(L, w0)]</span><br><span class="line"><span class="keyword">for</span> wi <span class="keyword">in</span> w:</span><br><span class="line">    KT_conditions.append(sympy.diff(L, wi))</span><br><span class="line"><span class="comment"># w0 Conditions</span></span><br><span class="line">basis_conditions = [w0]</span><br><span class="line"><span class="comment"># Total Conditions</span></span><br><span class="line">conditions = KT_conditions + relaxtion_conditions + basis_conditions</span><br><span class="line"></span><br><span class="line"><span class="comment"># Solve</span></span><br><span class="line">result = sympy.solve(conditions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find all solutions besides 0 solution</span></span><br><span class="line">w_sets = <span class="built_in">set</span>()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">    w_set = [i[wi] <span class="keyword">for</span> wi <span class="keyword">in</span> w]</span><br><span class="line">    <span class="comment"># Besides 0 solution</span></span><br><span class="line">    <span class="keyword">if</span> w_set.count(<span class="number">0</span>) == <span class="built_in">len</span>(w_set):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    w_sets.add(<span class="built_in">tuple</span>(w_set))</span><br><span class="line"><span class="built_in">print</span>(w_sets)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&#123;(1, 0)&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>
<figure>
<img src="/linear-discriminant-function/支持向量机的解.png" alt="支持向量机的解">
<figcaption aria-hidden="true">支持向量机的解</figcaption>
</figure>
<blockquote>
<p>以下是等价问题的转换。</p>
</blockquote>
<p>由基本的支持向量机公式，我们有拉格朗日函数 <span class="math display">\[
L(\boldsymbol{w},\lambda_i)=\frac{1}{2}\|\boldsymbol{w}\|^2-\sum^{l}_{i=1}\lambda_i\nabla(z_kg(\boldsymbol{x}_i)-1)
\]</span> 令拉格朗日函数偏导为零，代入公式可得 <span class="math display">\[
\max_\boldsymbol{\lambda}
L(\boldsymbol{\lambda})=\sum_{i=1}^l\lambda_i-\frac{1}{2}\sum^l_{k,j=1}\lambda_k\lambda_jz_kz_j\boldsymbol{x}_k^T\boldsymbol{x}_j\\
\text{s.t. }\sum_{i=1}^l\lambda_iz_i=0,\lambda_i\geq 0
\]</span> 即等价于二次规划问题。</p>
<p>比如说我们有二分类问题，样本为 <span class="math display">\[
\omega_1:\boldsymbol{x}_1=(3,3),\boldsymbol{x}_2=(4,3)\\
\omega_2:\boldsymbol{x}_3=(1,1)
\]</span> 解 <span class="math display">\[
\min_\boldsymbol{\lambda}\frac{1}{2}\sum^l_{k,j=1}\lambda_k\lambda_jz_kz_j\boldsymbol{x}_k^T\boldsymbol{x}_j-\sum_{i=1}^l\lambda_i\\
=\frac{1}{2}(18\lambda_1^2+25\lambda_2^2+2\lambda_3^2+42\lambda_1\lambda_2-12\lambda_1\lambda_3-14\lambda_2\lambda_3)-\lambda_1-\lambda_2-\lambda_3\\
\text{s.t. }\lambda_1+\lambda_2-\lambda_3=0,\lambda_i\geq0,i=1,2,3
\]</span> 将约束条件 <span class="math inline">\(\lambda_3=\lambda_1+\lambda_2\)</span>
带回目标函数得到 <span class="math display">\[
s(\lambda_1,\lambda_2)=4\lambda_1^2+\frac{13}{2}\lambda_2^2+10\lambda_1\lambda_2-2\lambda_1-2\lambda_2
\]</span> 求 <span class="math inline">\(\nabla s=0\)</span>，得到 <span class="math inline">\(s(\lambda_1,\lambda_2)\)</span> 在 <span class="math inline">\((\frac{3}{2},-1)\)</span> 上取极值，但不满足 <span class="math inline">\(\lambda_i\geq 0\)</span>
的约束条件，故在边界上找最小值。</p>
<p>当 <span class="math inline">\(\lambda_1=0\)</span> 时，最小值 <span class="math inline">\(s(0,\frac{2}{13})=-\frac{2}{13}\)</span>；</p>
<p>当 <span class="math inline">\(\lambda_2=0\)</span> 时，最小值 <span class="math inline">\(s(\frac{1}{4},0)=-\frac{1}{4}\)</span>。</p>
<p>故取最小值 <span class="math inline">\(\lambda_1=\frac{1}{4},\lambda_2=0,\lambda_3=\frac{1}{4}\)</span>，这样，<span class="math inline">\(\boldsymbol{x}_1,\boldsymbol{x}_3\)</span>
为支持向量。</p>
<p>利用 KT 条件反求得到 <span class="math inline">\(w_1=w_2=\frac{1}{2},b=-2\)</span>，故分离超平面为
<span class="math display">\[
\frac{1}{2}x_1+\frac{1}{2}x_2-2=0
\]</span></p>
<h3 id="核函数">核函数</h3>
<p>对于线性不可分问题，我们需要将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分，而映射的特征是基于原本特征的<strong>数学意义上的拓展特征</strong>。</p>
<p>那么我们就可以将原本的特征向量作映射，将支持向量机的模型拓展为 <span class="math display">\[
\max_\boldsymbol{\lambda}
L(\boldsymbol{\lambda})=\sum_{i=1}^l\lambda_i-\frac{1}{2}\sum^l_{k,j=1}\lambda_k\lambda_jz_kz_j\phi(\boldsymbol{x}_k)^T\phi(\boldsymbol{x}_j)\\
\text{s.t. }\sum_{i=1}^l\lambda_iz_i=0,\lambda_i\geq 0
\]</span> 其中，<span class="math inline">\(\phi(\boldsymbol{x})\)</span>
是映射函数，也称核映射，即将 <span class="math inline">\(l\)</span>
维的特征空间映射到更高维的特征空间。</p>
<p>那么分离超平面则为 <span class="math display">\[
f(\boldsymbol{x})=\boldsymbol{w}\phi(\boldsymbol{x})+b
\]</span> <img src="/linear-discriminant-function/核映射的一个示意图.png" alt="核映射的一个示意图"></p>
<p>比如说一个可能的核映射函数是从 <span class="math inline">\(R^2\to
R^3\)</span>，即 <span class="math display">\[
(x_1,x_2)\to(z_1,z_2,z_3)=(x_1^2,\sqrt{2}x_1x_2,x_2^2)
\]</span> <img src="/linear-discriminant-function/一个二维到三维的核映射.png" alt="一个二维到三维的核映射"></p>
<p>但我们知道，如果计算核映射，且计算高纬度的核映射，我们不仅需要设计一个好的核映射，还必须快速的去计算它，这是复杂的。</p>
<p>那么，我们可以发现，在约束中实际上核映射后的结果仅以内积的形式呈现，不妨我们设计一个核函数
<span class="math display">\[
k(\boldsymbol{x}_i,\boldsymbol{x}_j)=\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)
\]</span>
这样我们就可以<strong>不显式地设计核映射，而是设计核函数</strong>。（基本想法）</p>
<p>比如对于上面例子中的核映射，我们可以计算得到其核函数为 <span class="math display">\[
k(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\boldsymbol{x}_i^2+\boldsymbol{x}_j^2)^2
\]</span> <img src="/linear-discriminant-function/核函数的一个设计实例.png" alt="核函数的一个设计实例"></p>
<figure>
<img src="/linear-discriminant-function/常用核函数.png" alt="常用核函数">
<figcaption aria-hidden="true">常用核函数</figcaption>
</figure>
<h3 id="核支持向量机">核支持向量机</h3>
<p>基于核函数的思想，我们可以得到一个新的支持向量机模型，即核支持向量机模型
<span class="math display">\[
\max_\boldsymbol{\lambda}
L(\boldsymbol{\lambda})=\sum_{i=1}^l\lambda_i-\frac{1}{2}\sum^l_{k,j=1}\lambda_k\lambda_jz_kz_jk(\boldsymbol{x}_k,\boldsymbol{x}_j)\\
\text{s.t. }\sum_{i=1}^l\lambda_iz_i=0,\lambda_i\geq 0
\]</span> 例如对于在二位特征平面上线性不可分问题 XOR
问题，我们可以设计一个核函数为 <span class="math display">\[
k(\boldsymbol{x}_i,\boldsymbol{x}_j)=[1+\boldsymbol{x}_i^T\boldsymbol{x}_j]^2
\]</span> 使其在该空间上是线性可分的，一个可以构成这个核函数的核映射为
<span class="math display">\[
\phi(\boldsymbol{x})=(1,\sqrt{2}x_1,\sqrt{2}x_2,\sqrt{2}x_1x_2,x_1^2,x_2^2)
\]</span> XOR 问题中， <span class="math display">\[
\omega_1:\boldsymbol{x}_1=(1,1),\boldsymbol{x}_3=(-1,-1)\\
\omega_2:\boldsymbol{x}_2=(1,-1),\boldsymbol{x}_4=(-1,1)
\]</span> 计算过程忽略，我们可以得到 <span class="math display">\[
L(\boldsymbol{\lambda})=\lambda_1+\lambda_2+\lambda_3+\lambda_4\\-\frac{1}{2}(9\lambda_1^2-2\lambda_1\lambda_2-2\lambda_1\lambda_3+2\lambda_1\lambda_4\\+9\lambda_2^2+2\lambda_2\lambda_3-2\lambda_2\lambda_4\\+9\lambda_3^2-2\lambda_3\lambda_4\\+9\lambda_4^2)
\]</span> 最终得到 <span class="math inline">\(\lambda_1=\lambda_2=\lambda_3=\lambda_4=\frac{1}{8}\)</span>，四个样本均为支持向量。</p>
<p>最终解得最优分离超平面为 <span class="math display">\[
f(\boldsymbol{x})=-x_1x_2=0
\]</span></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>qsdz
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://hasegawaazusa.github.io/linear-discriminant-function.html" title="模式识别-线性判别函数笔记">https://hasegawaazusa.github.io/linear-discriminant-function.html</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/" rel="tag"><i class="fa fa-tag"></i> 模式识别</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/discrete-logarithm-note.html" rel="prev" title="离散对数笔记">
                  <i class="fa fa-chevron-left"></i> 离散对数笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/control-system-mathematical-model.html" rel="next" title="自动控制原理-控制系统数学模型">
                  自动控制原理-控制系统数学模型 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2022 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder"></span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">545k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8:15</span>
  </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
